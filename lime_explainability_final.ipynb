{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl132PDOOgjk"
      },
      "outputs": [],
      "source": [
        "#import model\n",
        "!pip install lime\n",
        "!pip install tensorflow transformers\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD IN MODEL (if needed)\n",
        "\n",
        "##One thing is check that the classification layer is of size 5 and not size 6.\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "\n",
        "checkpoint = 'roberta-base'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "file_path = \"insert path\"\n",
        "\n",
        "# Load the model\n",
        "roberta_classification_model = tf.keras.models.load_model(file_path, custom_objects={'TFRobertaModel': TFRobertaModel})\n",
        "\n",
        "# Display the model summary\n",
        "roberta_classification_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ArkEYGOnlr",
        "outputId": "c85c8ff0-dfca-4f30-8ebf-34f6de716437",
        "collapsed": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLaye  [(None, None)]               0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (Inpu  [(None, None)]               0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_roberta_model_1 (TFRobe  TFBaseModelOutputWithPooli   1246456   ['input_ids_layer[0][0]',     \n",
            " rtaModel)                   ngAndCrossAttentions(last_   32         'attention_mask_layer[0][0]']\n",
            "                             hidden_state=(None, None,                                            \n",
            "                             768),                                                                \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, None, 256)            590080    ['tf_roberta_model_1[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, None, 256)            0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, None, 256)            0         ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)        (None, 256)                  0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 6)                    1542      ['dropout_38[0][0]']          \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125762566 (479.75 MB)\n",
            "Trainable params: 125762566 (479.75 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_data = pd.read_csv('insert_path/efcamdat_final.csv')\n",
        "\n",
        "test_data['cefr_numeric'] = test_data['cefr_numeric'].astype('category')\n",
        "\n",
        "test_texts = test_data[\"text\"]\n",
        "test_labels = test_data[\"cefr_numeric\"]"
      ],
      "metadata": {
        "id": "KUAfDrGROnz7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_data[\"cefr_numeric\"].value_counts()\n",
        "num_classes = test_labels.nunique()\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8VzZEtZFsvi",
        "outputId": "c16f9723-0bcf-4f02-cff9-1784eb1a903c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_instance_index = 1\n",
        "test_instance = test_texts.iloc[test_instance_index]\n",
        "true_label = test_labels.iloc[test_instance_index]"
      ],
      "metadata": {
        "id": "B-iUR_tSOoJF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text: {test_instance_index} True Label: {true_label}\")"
      ],
      "metadata": {
        "id": "CQPBtB0u-ijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "test_instance_no_stopwords = remove_stopwords(test_instance)"
      ],
      "metadata": {
        "id": "l3vgPfxpeUZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc50914a-ffb7-4545-9d02-8f1d01442332"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_instance_no_stopwords)"
      ],
      "metadata": {
        "id": "PRCGYEunh2td",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6581c6d7-cd43-417a-9256-f1853773fd4b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bowling alley 8 meters x 3 meters line ten plastic bottles filled water. Put bottles rows 4, 3, 2, 1. Give player frisbee. Every player two shots turn. Give total ten turns each. Score pin, knocked one point. boottles knocked down, two shots points added together. plyer points win.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "\n",
        "def predict_proba(texts):\n",
        "    encodings = roberta_tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_SEQUENCE_LENGTH, return_tensors='tf')\n",
        "    inputs = {\n",
        "        'input_ids_layer': encodings['input_ids'],\n",
        "        #'token_type_ids': encodings['token_type_ids'],\n",
        "        'attention_mask_layer': encodings['attention_mask']\n",
        "    }\n",
        "    predictions = roberta_classification_model(inputs, training=False)\n",
        "    return tf.nn.softmax(predictions, axis=-1).numpy()\n",
        "\n",
        "# Get the predicted label for the test instance\n",
        "predicted_probs = predict_proba([test_instance_no_stopwords])\n",
        "predicted_label = predicted_probs[0].argmax()\n",
        "\n",
        "print(predicted_probs)\n",
        "#print(true_label)"
      ],
      "metadata": {
        "id": "4TzDpuYPOogZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f322e902-75d6-46c1-a96c-8f214ff8969a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.12958567 0.12961154 0.3520159  0.12961455 0.12959173 0.12958057]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Initialize the LIME text explainer\n",
        "explainer = LimeTextExplainer(class_names=list(test_labels.cat.categories))\n",
        "\n",
        "# Explain the model's prediction on the test instance\n",
        "exp = explainer.explain_instance(test_instance_no_stopwords, predict_proba, num_features=5)\n",
        "\n",
        "# Display the explanation with true and predicted labels\n",
        "print(f\"True label: {test_labels.cat.categories[true_label]}\")\n",
        "print(f\"Predicted label: {test_labels.cat.categories[predicted_label]}\")\n",
        "exp.show_in_notebook(text=True)"
      ],
      "metadata": {
        "id": "cHFYhvtCOy9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "weights = OrderedDict(exp.as_list())\n",
        "lime_weights = pd.DataFrame({\"words\": list(weights.keys()), \"weights\": list(weights.values())})\n",
        "\n",
        "sns.barplot(x = \"words\", y = \"weights\", data = lime_weights)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.title(\"Sample {} features weights given by LIME\".format(test_instance_index))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JFkOZyBGOzRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbfkcantkJ-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}