{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHCWTR895IgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464ccef2-d61f-48d2-9912-b45544233e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4CIMAWZwlYC",
        "outputId": "c842babd-10d9-4b73-9a26-65cf2da7e9c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import gc\n",
        "import textstat\n",
        "\n",
        "file_path = '/content/drive/My Drive/266 Project/efcamdat_sub.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poskK20S6plq",
        "outputId": "8bb51279-3665-4f95-e02c-89fef810003b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id  level  unit  learner_id learner_nationality  grade  \\\n",
            "0  679604      7     3      114335                  br     90   \n",
            "1  151196      9     2      136139                  sa     94   \n",
            "2  117084      9     4       34715                  br     88   \n",
            "3  113857      7     6       90269                  fr     90   \n",
            "4   22083      9     3       48465                  br     94   \n",
            "\n",
            "                      date  topic_id  \\\n",
            "0  2013-11-09 20:50:31.707        51   \n",
            "1  2012-09-25 06:01:08.117        66   \n",
            "2  2011-08-28 08:01:15.677        68   \n",
            "3  2011-07-31 14:51:22.547        54   \n",
            "4  2011-08-31 16:41:04.210        67   \n",
            "\n",
            "                                                text  cefr_numeric  \\\n",
            "0  From:l AS xxx@hotmail.com To: AS xxx@IXW.corpo...             3   \n",
            "1  I am so glad to receive this email from you. A...             3   \n",
            "2  Hi Fun Skydive, so I give up of my idea. I und...             3   \n",
            "3  Dear James, Some serious problems have been br...             3   \n",
            "4  Dear Sue, Thank you to interest in our product...             3   \n",
            "\n",
            "   cefr_grouped  \n",
            "0             2  \n",
            "1             2  \n",
            "2             2  \n",
            "3             2  \n",
            "4             2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check data**"
      ],
      "metadata": {
        "id": "iBD4pUDwa531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPyteXVE_sd",
        "outputId": "f8ecd882-4574-4716-a542-5a749d999de9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'level', 'unit', 'learner_id', 'learner_nationality', 'grade',\n",
              "       'date', 'topic_id', 'text', 'cefr_numeric', 'cefr_grouped'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFLKk4FKFQoW",
        "outputId": "65dfdc93-18f5-48bb-cf7c-56e3b3e54d15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "377967"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values\n",
        "df['labels'] = df['cefr_numeric'].apply(lambda x: x-1)"
      ],
      "metadata": {
        "id": "83ZJX73Z2TMf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out invalid labels\n",
        "df = df[df['labels'].isin(range(6))]"
      ],
      "metadata": {
        "id": "KnMaeqfW7f-7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values after filtering\n",
        "print(\"Unique label values after filtering:\", df['labels'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ew-wbO2TXf",
        "outputId": "98449fbd-76e5-4b05-89cd-f9b3ee665696"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values after filtering: [2 1 0 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "_G-sPwBgFWCs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation and test set sizes\n",
        "test_size = int(0.1 * len(df))  # 10% for testing\n",
        "valid_size = int(0.2 * len(df))  # 20% for validation"
      ],
      "metadata": {
        "id": "5lAedvCtwHB2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split off the test set\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=test_size, shuffle=True, random_state=42)\n",
        "\n",
        "# Split off the validation set from the remaining training data\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=valid_size, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "Br3teBflFh06"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique label values in training set\n",
        "print(\"Unique label values in training set:\", train_labels.unique())\n",
        "print(\"Unique label values in validation set:\", valid_labels.unique())\n",
        "print(\"Unique label values in test set:\", test_labels.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T4BSTSW_Yw6",
        "outputId": "d6afe4ed-ba88-458f-d393-4c18353956bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values in training set: [2 0 1 3 4 5]\n",
            "Unique label values in validation set: [2 0 3 1 4 5]\n",
            "Unique label values in test set: [5 1 2 3 0 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divide the data into training, validation, and test sets**"
      ],
      "metadata": {
        "id": "lrUsyA9NdhxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the size of the training and validation sets for faster processing\n",
        "train_texts = train_texts[:500]\n",
        "train_labels = train_labels[:500]\n",
        "valid_texts = valid_texts[:100]\n",
        "valid_labels = valid_labels[:100]\n",
        "test_texts = test_texts[:100]\n",
        "test_labels = test_labels[:100]"
      ],
      "metadata": {
        "id": "vKGNQfQ66qTa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract SMOG Index scores\n",
        "def extract_smog_features(texts):\n",
        "    smog_scores = [textstat.smog_index(text) for text in texts]\n",
        "    return smog_scores\n"
      ],
      "metadata": {
        "id": "0ZXCYIHCPIGU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_smog_scores = extract_smog_features(train_texts)\n",
        "valid_smog_scores = extract_smog_features(valid_texts)\n",
        "test_smog_scores = extract_smog_features(test_texts)"
      ],
      "metadata": {
        "id": "q-DcquR6PIPs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "max_length = 50\n",
        "rtokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "0Cw1fj_Ew3Px",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eeb105c-19d7-4d46-897c-527ac44103f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_length):\n",
        "    if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "        raise ValueError(\"Input texts should be a list of strings.\")\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "JseLIcudw6oe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_tokenize(texts, tokenizer, max_length):\n",
        "    try:\n",
        "        encodings = tokenize_texts(texts, tokenizer, max_length)\n",
        "        if 'input_ids' not in encodings:\n",
        "            raise ValueError(\"Tokenization did not produce 'input_ids'.\")\n",
        "        print(f\"Successfully tokenized {len(texts)} texts.\")\n",
        "        return encodings\n",
        "    except ValueError as e:\n",
        "        print(f\"Tokenization error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "BGP0hi02w6wz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = safe_tokenize(list(train_texts), rtokenizer, max_length)\n",
        "valid_encodings = safe_tokenize(list(valid_texts), rtokenizer, max_length)\n",
        "test_encodings = safe_tokenize(list(test_texts), rtokenizer, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxezpDYQw69T",
        "outputId": "313d6094-d4e7-4a7e-97a1-f21ddb8567f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized 500 texts.\n",
            "Successfully tokenized 100 texts.\n",
            "Successfully tokenized 100 texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_encodings is None or valid_encodings is None or test_encodings is None:\n",
        "    raise ValueError(\"Tokenization failed for one or more datasets.\")\n",
        "\n",
        "# Print the keys of the tokenized encodings to verify\n",
        "print(f\"Keys of train_encodings: {train_encodings.keys()}\")\n",
        "print(f\"Keys of valid_encodings: {valid_encodings.keys()}\")\n",
        "print(f\"Keys of test_encodings: {test_encodings.keys()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flCVqMlaxHMr",
        "outputId": "24aeace0-12e0-440c-80bd-8effa5488625"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of train_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of valid_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of test_encodings: dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Datasets with SMOG Index scores\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask, \"smog_score\": train_smog_scores},\n",
        "    train_labels\n",
        ")).shuffle(len(train_texts)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask, \"smog_score\": valid_smog_scores},\n",
        "    valid_labels\n",
        ")).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": test_encodings.input_ids, \"attention_mask_layer\": test_encodings.attention_mask, \"smog_score\": test_smog_scores},\n",
        "    test_labels\n",
        ")).batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "-NUCrWL3xHU_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check TensorFlow Dataset Elements\n",
        "for batch in test_dataset.take(1):\n",
        "    inputs, labels = batch\n",
        "    print(f\"Input IDs shape: {inputs['input_ids_layer'].shape}\")\n",
        "    print(f\"Attention Mask shape: {inputs['attention_mask_layer'].shape}\")\n",
        "    print(f\"SMOG Score shape: {inputs['smog_score'].shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    print(f\"Label values: {labels.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p12VLhQoxHXi",
        "outputId": "befa1966-8d98-4b53-cfa7-93bf2e64a568"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: (16, 50)\n",
            "Attention Mask shape: (16, 50)\n",
            "SMOG Score shape: (16,)\n",
            "Labels shape: (16,)\n",
            "Label values: [5 1 1 2 3 2 1 0 0 3 0 0 4 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture**"
      ],
      "metadata": {
        "id": "l20BGOsIE-Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def create_roberta_cl_model(model, num_classes=6, dropout=0.3, learning_rate=0.0001):\n",
        "    model.trainable = False\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "    smog_score = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='smog_score')  # Changed from 'ari_score' to 'smog_score'\n",
        "\n",
        "    model_inputs = [input_ids, attention_mask, smog_score]\n",
        "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    model_out = model_out.last_hidden_state\n",
        "\n",
        "    conv = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(model_out)\n",
        "    conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
        "    conv = tf.keras.layers.Dropout(dropout)(conv)\n",
        "    lstm = tf.keras.layers.LSTM(units=256, return_sequences=False, return_state=False)(conv)\n",
        "    lstm = tf.keras.layers.Dropout(dropout)(lstm)\n",
        "\n",
        "    # Concatenate LSTM output with the SMOG score\n",
        "    concatenated = tf.keras.layers.Concatenate()([lstm, smog_score])  # Changed from 'ari_score' to 'smog_score'\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(concatenated)\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=model_inputs, outputs=classification)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=0.1)\n",
        "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "    classification_model.compile(optimizer=optimizer,\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model"
      ],
      "metadata": {
        "id": "bt9I4AIbPIV3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "roberta_cl_model = create_roberta_cl_model(model=roberta_model, num_classes=6)"
      ],
      "metadata": {
        "id": "1HT7DcTbPIY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3014e8a-d512-482b-d3a4-656b8fe08dc4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear session and free memory before starting the training\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu7TZwPWxWt2",
        "outputId": "cba902c6-436f-4c29-85ce-51ae89670102"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21227"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model summary\n",
        "roberta_cl_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWYVZ_M6xbTp",
        "outputId": "795f0676-0e66-48c4-f0a1-ae4a052deef8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLaye  [(None, 50)]                 0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (Inpu  [(None, 50)]                 0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1246456   ['input_ids_layer[0][0]',     \n",
            " aModel)                     ngAndCrossAttentions(last_   32         'attention_mask_layer[0][0]']\n",
            "                             hidden_state=(None, 50, 76                                           \n",
            "                             8),                                                                  \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 48, 256)              590080    ['tf_roberta_model[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 24, 256)              0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 24, 256)              0         ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)        (None, 256)                  0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " smog_score (InputLayer)     [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 257)                  0         ['dropout_38[0][0]',          \n",
            "                                                                     'smog_score[0][0]']          \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 6)                    1548      ['concatenate[0][0]']         \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125762572 (479.75 MB)\n",
            "Trainable params: 1116940 (4.26 MB)\n",
            "Non-trainable params: 124645632 (475.49 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask}, train_labels))\n",
        "# dataset = dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask}, valid_labels))\n",
        "vdataset = vdataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "-y80hYeD1Yu0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "roberta_cl_model_history = roberta_cl_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScQZl-BxqIC",
        "outputId": "38193355-41b0-4ea4-b2af-f3dbab3b1bfd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 179s 4s/step - loss: 1.7198 - accuracy: 0.3060 - val_loss: 1.5685 - val_accuracy: 0.2900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model\n",
        "preds = roberta_cl_model.predict(test_dataset)\n",
        "pred_labels = tf.argmax(preds, axis=-1)\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "metadata": {
        "id": "ltk4d99FxqWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b26716-6cdc-45ae-d519-f125d2b45a6f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 29s 2s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.85      0.59        26\n",
            "           1       0.11      0.17      0.14        24\n",
            "           2       0.47      0.28      0.35        25\n",
            "           3       0.00      0.00      0.00        18\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.33       100\n",
            "   macro avg       0.17      0.22      0.18       100\n",
            "weighted avg       0.26      0.33      0.27       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqrxeeR4xqYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hFTdqlCxqb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}