{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6fea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cd393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7f9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d965502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../efcamdat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d85b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['writing_id', 'learner_id', 'learner_id_categorical', 'nationality',\n",
       "       'l1', 'cefr', 'cefr_numeric', 'level', 'unit', 'topic_id_original',\n",
       "       'topic_id_original_categorical', 'topic_id', 'topic_id_categorical',\n",
       "       'text_number_per_learner_in_task', 'topic', 'secondary_topic',\n",
       "       'topic_to_keep', 'date', 'time', 'grade', 'wordcount', 'mtld', 'text',\n",
       "       'text_corrected'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44219063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406062"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bea4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text']\n",
    "labels = df['cefr_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02526402",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_texts, test_texts, train_labels, test_labels) = train_test_split(text, labels, test_size=.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666a04d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40607"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0713a8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291489    \\n\\t  Crime: House theft Time: Monday 22th, Ju...\n",
       "389649    \\n\\t  Dear Mr. Harry Martin, I have long aspir...\n",
       "208666    \\n\\t  My name is Ivan and I'm a programmer. I ...\n",
       "105479    \\n\\t  Hugo's birthday, Hugo's party. Come to m...\n",
       "26899     \\n\\t  Hello teacher, How are you? My name's Mi...\n",
       "                                ...                        \n",
       "158628    \\n\\t  There are five people in my family. I'm ...\n",
       "390076    \\n\\t  The first property is a recently renovat...\n",
       "178737    \\n\\t  Name: Willian  Age: 29  Birthday : 30 Ma...\n",
       "255783    \\n\\t  My name is Amedeo Da Ros. I was born in ...\n",
       "277702    \\n\\t  Monkeys are so beautiful.they live in th...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9975093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_texts = train_texts.loc[:40607]\n",
    "# valid_labels = train_labels.loc[:40607]\n",
    "# train_texts = train_texts.loc[40607:]\n",
    "# train_labels = train_labels.loc[40607:]\n",
    "\n",
    "#smaller \n",
    "valid_texts_s = train_texts.iloc[:1000]\n",
    "valid_labels_s = train_labels.iloc[:1000]\n",
    "train_texts_s = train_texts.iloc[1000:11000]\n",
    "train_labels_s = train_labels.iloc[1000:11000]\n",
    "test_texts_s = test_texts.iloc[:1000]\n",
    "test_labels_s = test_labels.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "652c5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "m = \"intfloat/multilingual-e5-large-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "model = TFAutoModel.from_pretrained(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "aa6d477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5c646bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "valid_encodings = tokenizer(list(valid_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "test_encodings = tokenizer(list(test_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b863cd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
       "array([[     0,  64672,  62163,   1257,     99,    483,     36,     25,\n",
       "           238,  21135,      5,   4687,  60899,     47,   4488,     99,\n",
       "          3569,   1837,      5,   4687,   1556,  40781,     99,    427,\n",
       "            36,     25,    238,  21135,      5,  20414,  60899,     47,\n",
       "          5368,     99,   2289,   1837,      5,    360,     70, 105216,\n",
       "             4,   2412,  39544,     90,   1910,    136,   2412,  11301,\n",
       "             7,  69686,      5,   4687,   1556,  94000,     99, 159968,\n",
       "             5,   4687,  60899,     47,  11958,     99,    483,     36,\n",
       "            25,    238,  21135,      5,      2,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1,\n",
       "             1,      1,      1,      1,      1,      1,      1,      1]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.input_ids[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cd79dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiclass_model(model,\n",
    "                             num_classes = 5,\n",
    "                             hidden_size = 128,\n",
    "                             dropout=0.3,\n",
    "                             learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.traimable = True\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids_layer')\n",
    "    #token_type_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    model_inputs = [input_ids, attention_mask]\n",
    "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pooler_token = model_out[1]\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
    "    hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "\n",
    "\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[classification])\n",
    "\n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f5049be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = create_multiclass_model(model=model, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aaa19e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids_layer (InputLayer)   [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask_layer (InputLay  [(None, None)]      0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tfxlm_roberta_model (TFXLMRobe  TFBaseModelOutputWi  559890432  ['input_ids_layer[0][0]',        \n",
      " rtaModel)                      thPoolingAndCrossAt               'attention_mask_layer[0][0]']   \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 1024),                                                          \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 128)          131200      ['tfxlm_roberta_model[0][1]']    \n",
      "                                                                                                  \n",
      " dropout_296 (Dropout)          (None, 128)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 5)            645         ['dropout_296[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 560,022,277\n",
      "Trainable params: 560,022,277\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89dd8792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 20:44:35.791985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 144/1250 [==>...........................] - ETA: 3:49:07 - loss: 1.4807 - accuracy: 0.3872"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_model_history \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_encodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_encodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtrain_labels_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_encodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_encodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mvalid_labels_s\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_model_history = test_model.fit([train_encodings.input_ids, train_encodings.attention_mask],\n",
    "                                      train_labels_s,\n",
    "                                      validation_data=([valid_encodings.input_ids, valid_encodings.attention_mask],\n",
    "                                      valid_labels_s),\n",
    "                                      batch_size=8,\n",
    "                                      epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6c14b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7422d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "OneStopEnglishCorpus/Texts-SeparatedByReadingLevel\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "df2 = pd.DataFrame(columns=[['text', 'labels']])\n",
    "d = \"OneStopEnglishCorpus/Texts-SeparatedByReadingLevel\"\n",
    "\n",
    "for level in os.listdir(d):\n",
    "    try:\n",
    "        for f in os.listdir(os.path.join(d, level)):\n",
    "            try:\n",
    "                temp = pd.read_csv(os.path.join(d, level, f), delimiter='\\t', header=None, encoding='ISO-8859-1')\n",
    "                temp.columns = [['text']]\n",
    "                temp['labels'] = level\n",
    "                df2 = pd.concat([df2, temp])\n",
    "            except:\n",
    "                print(f)\n",
    "    except:\n",
    "        print(d)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "572bd37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    To tourists, Amsterdam still seems very libera...\n",
       "1    The Mayor, Eberhard van der Laan, says his new...\n",
       "2    Bartho Boer, a spokesman for the Mayor, says t...\n",
       "3    People found guilty of violent harassment will...\n",
       "4    One Dutch newspaper wrote that in the 19th cen...\n",
       "5    They are “scum houses” not scum villages, says...\n",
       "6    Police will watch the temporary accommodation,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = \"OneStopEnglishCorpus/Texts-Together-OneCSVperFile\"\n",
    "# dfs = []\n",
    "# for f in os.listdir(d):\n",
    "#     file_path = os.path.join(d, f)\n",
    "#     if os.path.isfile(file_path) and f.endswith('.csv'):\n",
    "#         try:\n",
    "#             temp = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "#             temp.columns = [\"Elementary\", \"Intermediate\", \"Advanced\"]\n",
    "#             dfs.append(temp)\n",
    "#         except:\n",
    "#             print(f)\n",
    "\n",
    "# df2 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "pd.read_csv(os.path.join(\"OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/Ele-Txt/Amsterdam-ele.txt\"), delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d58e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain-int.txt\n",
      "Royal Baby-int.txt\n",
      "WNL The lightweight-int.txt\n",
      "Arctic mapping-int.txt\n",
      "WNL Mystery Shopper-int.txt\n",
      "WNL Star Wars-int.txt\n",
      "WNL Four new elements-int.txt\n",
      "Int-Txt\n",
      "WNL Morocco-int.txt\n",
      "Zero Hours-int.txt\n",
      "Teff-int.txt\n",
      "WNL Scientist-int.txt\n",
      "WNL Are MOOCs the future-int.txt\n",
      "WNL Bangladeshi organization-int.txt\n",
      "Coal to challenge oil-int.txt\n",
      "WNL The age of music-int.txt\n",
      "WNL Google boss-int.txt\n",
      "Exercise-int.txt\n",
      "WNL On the trail-int.txt\n",
      "WNL SeaWorld-int.txt\n",
      "WNL Novel way-int.txt\n",
      "Malala-int.txt\n",
      "Lie detector-int.txt\n",
      "WNL shark-int.txt\n",
      "Syria-int.txt\n",
      "Japan-int.txt\n",
      "WNL New Orleans-int.txt\n",
      "Pope-int.txt\n",
      "WNL Google car-int.txt\n",
      "WNL First high resolution images-int.txt\n",
      "Billionaires-int.txt\n",
      "WNL Goodbye fish and chips-int.txt\n",
      ".DS_Store\n",
      "WNL Calais Migrants-int.txt\n",
      "WNL Extreme heat-int.txt\n",
      "WNL School-int.txt\n",
      "WNL Spain Robin Hood-int.txt\n",
      "WNL Revolution-int.txt\n",
      "WNL Vienna-int.txt\n",
      "Japan menu-int.txt\n",
      "Crowdfunding-int.txt\n",
      "Rats-int.txt\n",
      "Richard III-int.txt\n",
      "Organs-int.txt\n",
      "WNL Switched Babies-int.txt\n",
      "WNL Scarlett-int.txt\n",
      "WNL Poverty-int.txt\n",
      "Bonus pay-int.txt\n",
      "WNL Pizza chain-int.txt\n",
      "WNL David Mitchell-int.txt\n",
      "NSA 2-int.txt\n",
      "Nelson Mandela-int.txt\n",
      "Insects-int.txt\n",
      "WNL Apple-int.txt\n",
      "WNL Inventions-int.txt\n",
      "Thatcher-int.txt\n",
      "WNL Black Friday-int.txt\n",
      "WNL Five jobs-int.txt\n",
      "Superbugs-int.txt\n",
      "WNL Changing India-int.txt\n",
      "Ferguson-int.txt\n",
      "Midlife crisis-int.txt\n",
      "WNL Organic Food-int.txt\n",
      "WNL Why you should start work at 10-int.txt\n",
      "WNL Ways Facebook changed-int.txt\n",
      "WNL Waiters-int.txt\n",
      "WNL Tributes-int.txt\n",
      "Copyright-int.txt\n",
      "WNL Pacific Islanders-int.txt\n",
      "WNL Loneliness-int.txt\n",
      "WNL Who invented-int.txt\n",
      "WNL What is world's-int.txt\n",
      "WNL Bright Future-int.txt\n",
      "WNL How downtown Abbey-int.txt\n",
      "WNL Wealth therapy-int.txt\n",
      "Gorillas and oil-int.txt\n",
      "Life on Mars-int.txt\n",
      "Cigarettes-int.txt\n",
      "Anita-int.txt\n",
      "WNL Use of e-cig-int.txt\n",
      "Banksy-int.txt\n",
      "Bolivia-int.txt\n",
      "Violence-int.txt\n",
      "WNL Brown bears-int.txt\n",
      "WNL Neuroscientists-int.txt\n",
      "WNL We must-int.txt\n",
      "WNL Why we should -int.txt\n",
      "Nepal-int.txt\n",
      "Prince Harry-int.txt\n",
      "WNL EU Pollution-int.txt\n",
      "WNL I confused my numbers-int.txt\n",
      "Brazil-int.txt\n",
      "WNL First female coach-int.txt\n",
      "WNL School Sports-int.txt\n",
      "WNL Mars-int.txt\n",
      "WNL Love hormonw-int.txt\n",
      "WNL The millenials-int.txt\n",
      "WNL Who can stop Trump-int.txt\n",
      "WNL A good night sleep-int.txt\n",
      "Kashmir-int.txt\n",
      "WNL 101 year old bottle message-int.txt\n",
      "WNL Coloring Books-int.txt\n",
      "WNL Inky the octopus-int.txt\n",
      "WNL Noise pollution -int.txt\n",
      "WNL Mirrors-int.txt\n",
      "Murderers beware-int.txt\n",
      "WNL Making a killing-int.txt\n",
      "Skydiver-int.txt\n",
      "Blackberry-int.txt\n",
      "Old age-int.txt\n",
      "US shutdown-int.txt\n",
      "WNL Will drones-int.txt\n",
      "WNL Kiruna-int.txt\n",
      "Starbucks-int.txt\n",
      "WNL NASA Astronaut-int.txt\n",
      "WNL are you a bad boyfriend-int.txt\n",
      "WNL Kate Bush-int.txt\n",
      "WNL Canadian Pair-int.txt\n",
      "WNL Extinction-int.txt\n",
      "Denmark-int.txt\n",
      "WNL Icelandic names-int.txt\n",
      "WNL Six year olds-int.txt\n",
      "WNL Ships noise-int.txt\n",
      "Kate and William-int.txt\n",
      "WNL Satnav-int.txt\n",
      "WNL Shocking-int.txt\n",
      "Swedish prisons-int.txt\n",
      "Facebook deserted by millions of users-int.txt\n",
      "WNL John Lewis-int.txt\n",
      "Amsterdam-int.txt\n",
      "WNL David Bowie-int.txt\n",
      "Obama-int.txt\n",
      "Hillsborough-int.txt\n",
      "Amazon-int.txt\n",
      "WNL six well paid jobs-int.txt\n",
      "Fitness-int.txt\n",
      "WNL In flight-int.txt\n",
      "WNL Shops-int.txt\n",
      "WNL Billionaires-int.txt\n",
      "WNL Glastonbury-int.txt\n",
      "WNL JMW Turner-int.txt\n",
      "WNL Planet-int.txt\n",
      "WNL Young Americans-int.txt\n",
      "WNL Fifth of young adults-int.txt\n",
      "WNL Bangladesh factory owners-int.txt\n",
      "WNL Ten ideas-int.txt\n",
      "Everest-int.txt\n",
      "WNL Cuba-int.txt\n",
      "WNL Scottish referendum-int.txt\n",
      "WNL Icebucket challenge-int.txt\n",
      "WNL Still no flying cars-int.txt\n",
      "WNL Japan-int.txt\n",
      "WNL Spain Wetlands-int.txt\n",
      "WNL Swarthy caveman-int.txt\n",
      "Life expectancy-int.txt\n",
      "climate change -int.txt\n",
      "Food shortages-int.txt\n",
      "Greeks and drugs-int.txt\n",
      "False memory-int.txt\n",
      "WNL World Cup-int.txt\n",
      "WNL The Greek island-int.txt\n",
      "WNL Rwanda-int.txt\n",
      "WNL The last-int.txt\n",
      "WNL Bogus Allergy-int.txt\n",
      "NSA scandal-int.txt\n",
      "WNL What's the secret-int.txt\n",
      "WNL Drowning in rubbish-int.txt\n",
      "WNL FIFA-int.txt\n",
      "WNL Music-int.txt\n",
      "WNL Lego movie-int.txt\n",
      "WNL Nigerian low tech-int.txt\n",
      "Kenya-int.txt\n",
      "WNL Winter-int.txt\n",
      "WNL Criminal Links-int.txt\n",
      "WNL Men in black-int.txt\n",
      "WNL What does Apple-int.txt\n",
      "Norwegian sun-int.txt\n",
      "WNL Basic phone logs-int.txt\n",
      "WNL Shakespeare-int.txt\n",
      "WNL Why are people so wrong-int.txt\n",
      "WNL Arctic Ramadan-int.txt\n",
      "WNL Barack Obama-int.txt\n",
      "WNL Castaway-int.txt\n",
      "WNL Experience-int.txt\n",
      "WNL Man falls-int.txt\n",
      "Gangs-int.txt\n",
      "Liberia-int.txt\n",
      "Meteorite-int.txt\n",
      "Loterry-int.txt\n",
      "WNL India's rich-int.txt\n",
      "WNL Can the US-int.txt\n",
      "WNL SeaWorld-ele.txt\n",
      "Exercise-ele.txt\n",
      "WNL On the trail-ele.txt\n",
      "Malala-ele.txt\n",
      "WNL Novel way-ele.txt\n",
      "WNL shark-ele.txt\n",
      "Lie detector-ele.txt\n",
      "Japan-ele.txt\n",
      "Syria-ele.txt\n",
      "WNL Four new elements-ele.txt\n",
      "WNL Mystery Shopper-ele.txt\n",
      "Arctic mapping-ele.txt\n",
      "WNL The lightweight-ele.txt\n",
      "WNL Star Wars-ele.txt\n",
      "Royal Baby-ele.txt\n",
      "Spain-ele.txt\n",
      "Teff-ele.txt\n",
      "WNL Scientist-ele.txt\n",
      "WNL Are MOOCs the future-ele.txt\n",
      "WNL Morocco-ele.txt\n",
      "Zero Hours-ele.txt\n",
      "Coal to challenge oil-ele.txt\n",
      "WNL Bangladeshi organization-ele.txt\n",
      "WNL Google boss-ele.txt\n",
      "WNL The age of music-ele.txt\n",
      "Crowdfunding-ele.txt\n",
      "Japan menu-ele.txt\n",
      ".DS_Store\n",
      "Rats-ele.txt\n",
      "Richard III-ele.txt\n",
      "Bonus pay-ele.txt\n",
      "WNL Poverty-ele.txt\n",
      "WNL Scarlett-ele.txt\n",
      "WNL Switched Babies-ele.txt\n",
      "Organs-ele.txt\n",
      "WNL Pizza chain-ele.txt\n",
      "WNL David Mitchell-ele.txt\n",
      "WNL First high resolution images-ele.txt\n",
      "Pope-ele.txt\n",
      "WNL Google car-ele.txt\n",
      "WNL New Orleans-ele.txt\n",
      "WNL Extreme heat-ele.txt\n",
      "WNL Calais Migrants-ele.txt\n",
      "Billionaires-ele.txt\n",
      "WNL Goodbye fish and chips-ele.txt\n",
      "WNL School-ele.txt\n",
      "WNL Revolution-ele.txt\n",
      "WNL Vienna-ele.txt\n",
      "WNL Spain Robin Hood-ele.txt\n",
      "WNL Who invented-ele.txt\n",
      "WNL Loneliness-ele.txt\n",
      "WNL Pacific Islanders-ele.txt\n",
      "WNL Bright Future-ele.txt\n",
      "WNL What is world's-ele.txt\n",
      "Gorillas and oil-ele.txt\n",
      "WNL Wealth therapy-ele.txt\n",
      "WNL How downtown Abbey-ele.txt\n",
      "Banksy-ele.txt\n",
      "Bolivia-ele.txt\n",
      "Anita-ele.txt\n",
      "WNL Use of e-cig-ele.txt\n",
      "Life on Mars-ele.txt\n",
      "Cigarettes-ele.txt\n",
      "WNL Inventions-ele.txt\n",
      "Insects-ele.txt\n",
      "WNL Apple-ele.txt\n",
      "NSA 2-ele.txt\n",
      "Nelson Mandela-ele.txt\n",
      "WNL Five jobs-ele.txt\n",
      "WNL Black Friday-ele.txt\n",
      "Thatcher-ele.txt\n",
      "WNL Ways Facebook changed-ele.txt\n",
      "WNL Why you should start work at 10-ele.txt\n",
      "Midlife crisis-ele.txt\n",
      "WNL Organic Food-ele.txt\n",
      "Superbugs-ele.txt\n",
      "Ferguson-ele.txt\n",
      "WNL Changing India-ele.txt\n",
      "Copyright-ele.txt\n",
      "WNL Waiters-ele.txt\n",
      "WNL Tributes-ele.txt\n",
      "WNL Coloring Books-ele.txt\n",
      "WNL 101 year old bottle message-ele.txt\n",
      "Kashmir-ele.txt\n",
      "WNL Mirrors-ele.txt\n",
      "WNL Inky the octopus-ele.txt\n",
      "WNL Noise pollution -ele.txt\n",
      "Blackberry-ele.txt\n",
      "Old age-ele.txt\n",
      "Skydiver-ele.txt\n",
      "WNL Making a killing-ele.txt\n",
      "Murderers beware-ele.txt\n",
      "US shutdown-ele.txt\n",
      "WNL We must-ele.txt\n",
      "WNL Neuroscientists-ele.txt\n",
      "WNL Brown bears-ele.txt\n",
      "Violence-ele.txt\n",
      "Prince Harry-ele.txt\n",
      "Nepal-ele.txt\n",
      "WNL Why we should -ele.txt\n",
      "WNL I confused my numbers-ele.txt\n",
      "Brazil-ele.txt\n",
      "WNL EU Pollution-ele.txt\n",
      "WNL A good night sleep-ele.txt\n",
      "WNL Who can stop Trump-ele.txt\n",
      "WNL Love hormonw-ele.txt\n",
      "WNL Mars-ele.txt\n",
      "WNL The millenials-ele.txt\n",
      "WNL School Sports-ele.txt\n",
      "WNL First female coach-ele.txt\n",
      "WNL Ships noise-ele.txt\n",
      "WNL Six year olds-ele.txt\n",
      "WNL Satnav-ele.txt\n",
      "Kate and William-ele.txt\n",
      "Swedish prisons-ele.txt\n",
      "WNL Shocking-ele.txt\n",
      "WNL John Lewis-ele.txt\n",
      "Facebook deserted by millions of users-ele.txt\n",
      "WNL Kiruna-ele.txt\n",
      "Starbucks-ele.txt\n",
      "WNL Will drones-ele.txt\n",
      "WNL Kate Bush-ele.txt\n",
      "WNL NASA Astronaut-ele.txt\n",
      "WNL are you a bad boyfriend-ele.txt\n",
      "WNL Canadian Pair-ele.txt\n",
      "Denmark-ele.txt\n",
      "WNL Icelandic names-ele.txt\n",
      "WNL Extinction-ele.txt\n",
      "WNL Young Americans-ele.txt\n",
      "WNL Planet-ele.txt\n",
      "WNL JMW Turner-ele.txt\n",
      "WNL Glastonbury-ele.txt\n",
      "WNL Billionaires-ele.txt\n",
      "WNL Fifth of young adults-ele.txt\n",
      "WNL Bangladesh factory owners-ele.txt\n",
      "WNL Scottish referendum-ele.txt\n",
      "WNL Icebucket challenge-ele.txt\n",
      "WNL Cuba-ele.txt\n",
      "Everest-ele.txt\n",
      "WNL Ten ideas-ele.txt\n",
      "Obama-ele.txt\n",
      "Hillsborough-ele.txt\n",
      "WNL David Bowie-ele.txt\n",
      "Amsterdam-ele.txt\n",
      "Fitness-ele.txt\n",
      "WNL six well paid jobs-ele.txt\n",
      "Amazon-ele.txt\n",
      "WNL Shops-ele.txt\n",
      "WNL In flight-ele.txt\n",
      "WNL The Greek island-ele.txt\n",
      "WNL Bogus Allergy-ele.txt\n",
      "WNL The last-ele.txt\n",
      "WNL Rwanda-ele.txt\n",
      "WNL What's the secret-ele.txt\n",
      "NSA scandal-ele.txt\n",
      "WNL Drowning in rubbish-ele.txt\n",
      "WNL Japan-ele.txt\n",
      "WNL Still no flying cars-ele.txt\n",
      "Life expectancy-ele.txt\n",
      "climate change -ele.txt\n",
      "WNL Swarthy caveman-ele.txt\n",
      "WNL Spain Wetlands-ele.txt\n",
      "False memory-ele.txt\n",
      "Greeks and drugs-ele.txt\n",
      "Food shortages-ele.txt\n",
      "WNL World Cup-ele.txt\n",
      "Gangs-ele.txt\n",
      "WNL Man falls-ele.txt\n",
      "WNL Experience-ele.txt\n",
      "WNL Can the US-ele.txt\n",
      "WNL India's rich-ele.txt\n",
      "Loterry-ele.txt\n",
      "Meteorite-ele.txt\n",
      "Liberia-ele.txt\n",
      "WNL Lego movie-ele.txt\n",
      "WNL Nigerian low tech-ele.txt\n",
      "WNL Music-ele.txt\n",
      "WNL FIFA-ele.txt\n",
      "WNL What does Apple-ele.txt\n",
      "Kenya-ele.txt\n",
      "WNL Winter-ele.txt\n",
      "WNL Criminal Links-ele.txt\n",
      "WNL Men in black-ele.txt\n",
      "WNL Shakespeare-ele.txt\n",
      "Norwegian sun-ele.txt\n",
      "WNL Basic phone logs-ele.txt\n",
      "WNL Barack Obama-ele.txt\n",
      "WNL Castaway-ele.txt\n",
      "WNL Arctic Ramadan-ele.txt\n",
      "WNL Why are people so wrong-ele.txt\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(d):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(f)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/.DS_Store'"
     ]
    }
   ],
   "source": [
    "for level in os.listdir(d):\n",
    "    for f in os.listdir(os.path.join(d, level)):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "72d18a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52cfad",
   "metadata": {},
   "source": [
    "# Trying Roberta + CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ebe5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1bd3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rtokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f33b710f-09ba-42e6-beab-82c6ea45ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffba5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = rtokenizer(list(train_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "valid_encodings = rtokenizer(list(valid_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
    "test_encodings = rtokenizer(list(test_texts_s), truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78b6b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roberta_model(model,\n",
    "                         num_classes = 5,\n",
    "                         dropout=0.3,\n",
    "                         learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.traimable = True\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids_layer')\n",
    "    #token_type_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    model_inputs = [input_ids, attention_mask]\n",
    "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    model_out = model_out[0]\n",
    "\n",
    "    conv = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(model_out)\n",
    "    conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = tf.keras.layers.Dropout(dropout)(conv)\n",
    "    lstm = tf.keras.layers.LSTM(units=256, return_sequences=False, return_state=False)(conv)\n",
    "    lstm = tf.keras.layers.Dropout(dropout)(lstm)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(lstm)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[classification])\n",
    "\n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                 metrics='accuracy')\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeef4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model2 = create_roberta_model(model=roberta_model, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a330ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids_layer (InputLayer)   [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask_layer (InputLay  [(None, None)]      0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_ids_layer[0][0]',        \n",
      " odel)                          thPoolingAndCrossAt               'attention_mask_layer[0][0]']   \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, None, 256)    590080      ['tf_roberta_model_1[12][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, None, 256)   0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_94 (Dropout)           (None, None, 256)    0           ['max_pooling1d_7[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 256)          525312      ['dropout_94[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_95 (Dropout)           (None, 256)          0           ['lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 5)            1285        ['dropout_95[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,762,309\n",
      "Trainable params: 125,762,309\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "25820ada-af07-4e2c-ac06-9d700cc28a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    }
   ],
   "source": [
    "test_model_history = test_model2.fit([train_encodings.input_ids, train_encodings.attention_mask],\n",
    "                                      train_labels_s,\n",
    "                                      validation_data=([valid_encodings.input_ids, valid_encodings.attention_mask],\n",
    "                                      valid_labels_s),\n",
    "                                      batch_size=8,\n",
    "                                      epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
