{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHCWTR895IgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5742d3-f658-4bc6-b5b3-70c97540e9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "\n",
        "file_path = '/content/drive/My Drive/266 Project/efcamdat_valid_data.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n",
        "\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poskK20S6plq",
        "outputId": "9ed65b22-0b29-4e6e-b31f-a7f249a1e082"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                               text  labels\n",
            "0      144914  My name is Guo Jing and my english C English n...       1\n",
            "1      107289  I've traveled SP travelled to Venezuela. I've ...       1\n",
            "2      224725  My name is Nancy, AS I live in Wuhan. AS I'm t...       0\n",
            "3       19046  I just heard a song by Josh Woodward called 'W...       2\n",
            "4      231400  Hi, It's been brought to my attention that you...       0\n",
            "Index(['Unnamed: 0', 'text', 'labels'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check data**"
      ],
      "metadata": {
        "id": "iBD4pUDwa531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPyteXVE_sd",
        "outputId": "a188929a-1c03-47d0-d3eb-5d485280b8cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'text', 'labels'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFLKk4FKFQoW",
        "outputId": "2543fb5d-87e4-4452-8dcc-66d1d8450512"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40607"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique label values before filtering:\", df['labels'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6lD5Sn1rjyt",
        "outputId": "7adac24f-139e-4799-afd9-1a7e20054cbb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values before filtering: [1 0 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out invalid labels\n",
        "df = df[df['labels'].isin(range(6))]"
      ],
      "metadata": {
        "id": "LOyWeDjnsL_G"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values after filtering make sure 6 labels\n",
        "print(\"Unique label values after filtering:\", df['labels'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZcpzTGuu1E",
        "outputId": "1ff24b92-6a78-4a41-eec3-c0a3af88bb12"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values after filtering: [1 0 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation and test set sizes**"
      ],
      "metadata": {
        "id": "3494FE9o_ksP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "_G-sPwBgFWCs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation and test set sizes\n",
        "test_size = int(0.1 * len(df))  # 10% for testing\n",
        "valid_size = int(0.2 * len(df))  # 20% for validation"
      ],
      "metadata": {
        "id": "GZ-qWlhEv_2W"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split off the test set\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=test_size, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "Br3teBflFh06"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split off the validation set from the remaining training data\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=valid_size, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "mdBnKzJcwMt1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values\n",
        "print(\"Unique label values in training set:\", train_labels.unique())\n",
        "print(\"Unique label values in validation set:\", valid_labels.unique())\n",
        "print(\"Unique label values in test set:\", test_labels.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nQBfQ6fvEjo",
        "outputId": "301814bd-16eb-4549-b502-d1a31c16a597"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values in training set: [3 2 1 0 4 5]\n",
            "Unique label values in validation set: [1 3 2 0 4 5]\n",
            "Unique label values in test set: [3 0 2 4 1 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u4TqTRdFj8V",
        "outputId": "61f6b1b5-1a7a-4a79-ad8b-f8513f96b53b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4060"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "train_vectors = vectorizer.fit_transform(train_texts)\n",
        "test_vectors = vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "OiUDi99EMYba"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a logistic regression model\n",
        "baseline_model = LogisticRegression(max_iter=1000)\n",
        "baseline_model.fit(train_vectors, train_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "9U3EC4c2MYm2",
        "outputId": "a539f08d-030a-4b56-cdbb-2cff1f07defd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model**"
      ],
      "metadata": {
        "id": "CwpJqRMx_-1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = baseline_model.predict(test_vectors)\n",
        "print(classification_report(test_labels, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k8nMGpgMYwQ",
        "outputId": "a71c78d0-7bfa-4ac1-ac1c-63764f55b229"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96      1054\n",
            "           1       0.94      0.92      0.93      1124\n",
            "           2       0.89      0.94      0.92      1091\n",
            "           3       0.87      0.90      0.89       609\n",
            "           4       0.98      0.59      0.74       159\n",
            "           5       0.89      0.35      0.50        23\n",
            "\n",
            "    accuracy                           0.92      4060\n",
            "   macro avg       0.92      0.78      0.82      4060\n",
            "weighted avg       0.92      0.92      0.92      4060\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-check Label Values Before Creating TensorFlow Datasets**"
      ],
      "metadata": {
        "id": "61dZVjrhAI3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text'].tolist()\n",
        "labels = df['labels'].tolist()\n",
        "\n",
        "validation_proportion = 0.1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=0.1, shuffle=True, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "1RQFrczbN2Gt"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training set into a smaller training set and validation set\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.1, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "mhCJj38ZVIvs"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data\n",
        "print(f\"Total dataset size: {len(text)}\")\n",
        "print(f\"Training set size: {len(train_texts)}\")\n",
        "print(f\"Validation set size: {len(valid_texts)}\")\n",
        "print(f\"Test set size: {len(test_texts)}\")\n",
        "\n",
        "print(f\"First 5 elements of train_texts: {train_texts[:5]}\")\n",
        "print(f\"First 5 elements of valid_texts: {valid_texts[:5]}\")\n",
        "print(f\"First 5 elements of test_texts: {test_texts[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdZBY-5VI6q",
        "outputId": "e1a50125-a3a0-4d7c-a416-7caa50e93e85"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 40607\n",
            "Training set size: 32891\n",
            "Validation set size: 3655\n",
            "Test set size: 4061\n",
            "First 5 elements of train_texts: [\"I don''t want to name He is so inconvinient and he But he can''t imagine a girl put a video in the internet when he sleep and she became a 15 minutes celebrites. He get very nervous with that and the girl is getting money with this.\", \"I'm Anna I'm 40. I have long brown hair and big black eyes. I'm short and slim. I have AR a big RS nose and AR a small mouth. I'm a manager. AS I like my job very much. I have three pepole SP people in my family.%%My husband, AS my son and I. I love them very much.\", \"Hi, Mr. Souza . PU , I have great news about the meeting I had with Mr. Lucio, on the issue of buying him food company, AS '' RS OCP Alimentos''. They really showed interest in concluding the negotiations, may join our strength and our prestige to their company, but the values ??are still a bit high for us, so we have to work to decrease and leave a level that we cover. AS It is worth mentioning education, friendliness and level of professionalism of Lucio, who proved to be a really nice man to negotiate and with whom I established a high level of trading RS !\", \"Hi my friend, I Know C know that this situacion SP situation is very dificult SP difficult , but I need to do something with my job. In the future, I would like to do something like a graduation, to be D better my salary. I m SP I'm very happy with my live SP life , I love my Family C family and I need to be better Always C always .\", 'I was born in 1972. I got my first job when I was 15. I got my first car when I was 19 years old. I graduated from university when I was 21. After graduated I moved out of my parents house. Two years ago, I fell in love. Now we have plans for married VT marrying and having childrens SP children .']\n",
            "First 5 elements of valid_texts: [\"I work in an office as accountant. I like my job because there are always a lot of things to do, like talk on the phone, write email, make orders, write letters. I love to find always more convinient prices when it's time ti buy something. There are a lot of different ways to find them, on catalogues or on the internet. It's a really interesting job.\", 'My hometown is the place where the first classic violin was made and where the most famous violin maker was born. Since then, my city is the scenery of an unique performing art: the classical violin solo concert. Im proud to say that only in my hometown you can enjoy the sound of medieval violin in a ultra-modern concert hall designed specifically for amplifying the sound of violins. Its an enthusiastic experience, even for those who had never enjoyed classical music. The sound is exquisite and its so clear that you are able to distinguish every single note. I heavily recommend to anyone who is visiting my city for the first time to check is there is any violin audition available. On matter of fact, due to the rising numer SP number of violin maker PL makers the auditions are free or really cheap. However, despite the fact this performing act is unique and should be world-wide known, the local politician PL politicians arent able to sustain, to boost and to promote it. What a shame.', 'Hi, my name is Joselias. I married with aldaires since 1983. We have three children. I working in service public of Brazil. Two children are man and are makind of college. I have a children woman who also is studying of second degrees. My wife is teacher. She is making also of course of doctor. My family is very interesting and everybody are studying. I live in Vitria - Brazil.', \"I beliewe SP believe that in AR the future PU , we'll have a cure for sickness WC diseases MW such as cancer, better comunication SP communication between the D people, AS because PR of lack PR of dialogue today, maybe the D tecnology SP technology can help us, we'll NS us. We'll have vehicles MW that are more moderns WC modern , less politions WC polluting , fewer problems of public mobile WC transport , maybe green trains, green cars, more bicycles on the streets, more organics WC organic food, a health sistems public WO public health system witch SP with less waiting, more moderns WC modern medication, more evolution and innovation PR in telecomminications SP telecommunication , AS fewer stressed people. In general PU , I'm optimistic with WC about the future, I wait WC want that the D people MW will live better, I believe in us scientists for a lot PR of things.\", \"I C I arrived at home and found the window broked NSW broken. , D the C The door was opened, and I'm righ NSW sure that I locked MW it before MW I go VT went to work. RS I saw a man running away because he heard my footsteps. I look VT looked trough WC through the window and see VT saw a tall and black guy runnning SP running with a gun in his hand. He was carring SP carrying a beige backpack , PU . probably C Probably VT stuffed into it was my stuffs stolen WO stolen stuff .\"]\n",
            "First 5 elements of test_texts: [\"we C We 're in good shape financially now. my C My husband is great at managing his finances. AS our C Our savings have grown a lot these last few years .. PU , AS but in my country PU , many people have been suffering financialiy SP financially since many years PU . The living cost is increasing day by day and the currency is going down rapidly. AS prices C Prices are continually going up. AS they C They are not able to manage their mortagage SP mortgage . AS some C Some of them live in unplesent SP unpleasant circumstances where sometime PU , they have a small roof over their heads PU , but no food on the table and some time nothing. Providing more jobs and teach them how to manage money to make a buget SP budget might be a good idea to come over these problems.\", \"I have always tried to make the most of my life because I understand life is short so we have to waste WC spend our time the best we can. Every day is a gift that shouldn't be spent with stupid or unimportant things. The most important thing to me is to spend time with my family. I have a son, two daughthers SP daughters and a granddaughther SP granddaughter . I lost my father five years ago and a lovely aunt in the same year. It took me a very long time to deal with the death of both, but the most shocking event that was completely unexpected and occured SP occurred a year ago was the death of my so beloved sister. She had a breath and heart stop WC Her heart stopped beating and she stopped breathing . It was WC happened so suddently SP suddenly that I am still recovering of WC from the great shock. My mother is still dealing with this and is taking some medicine to help. I thank God every day for still having my mother alive. So, after all these events I definetely SP definitely decided that I love my family and spend WC spending time with them is the best thing to do. Although I am still working, I mean I am not retired yet, I always find time to see all of them almost every day.%%\", 'I live with Tamara Elisa. Her WC She is some D beautiful, very intelligent and funny. She is small WC short and RS have WC has brown RS eyes and a D long and D brown hair. We are a D good friends. I and her WC We are married at PR for five years.', \"Dear Suellem, You and I have been good moments for a long time and I need to be honest with you. I've decided to move to a AR an other city and get a new job. My dream has always been to work in a police station, maybe a cop. I need the support of my family to be a police man. Then PU , I can help a lot of people in my new city. Thanks for being such a good friend. Enio\", \"In my country, AS Brazil, the price of almost everything is going up. The transportation is going up, as the housing. Clothing is very expensive. In some regions of Brazil PU , the people aren't living well. The cost of living is very difficult . But CO , but the vegetable and fruit are decrease VT decreasing .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the validation and test set sizes\n",
        "valid_size = 40607\n",
        "test_size = 4061"
      ],
      "metadata": {
        "id": "O3DnAVpyVI9K"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "-pz26DI4Aou_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 50\n",
        "rtokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_texts(texts, tokenizer, max_length):\n",
        "    if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "        raise ValueError(\"Input texts should be a list of strings.\")\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "0rtQUfskNHWG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_tokenize(texts, tokenizer, max_length):\n",
        "    try:\n",
        "        encodings = tokenize_texts(texts, tokenizer, max_length)\n",
        "        if 'input_ids' not in encodings:\n",
        "            raise ValueError(\"Tokenization did not produce 'input_ids'.\")\n",
        "        print(f\"Successfully tokenized {len(texts)} texts.\")\n",
        "        return encodings\n",
        "    except ValueError as e:\n",
        "        print(f\"Tokenization error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "pSv6tNMUTLh_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = safe_tokenize(list(train_texts), rtokenizer, max_length)\n",
        "valid_encodings = safe_tokenize(list(valid_texts), rtokenizer, max_length)\n",
        "test_encodings = safe_tokenize(list(test_texts), rtokenizer, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojFqLtT0TLt_",
        "outputId": "2c3b71d6-9228-498d-8ca0-8cfdc4bc1002"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized 32891 texts.\n",
            "Successfully tokenized 3655 texts.\n",
            "Successfully tokenized 4061 texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_encodings is None or valid_encodings is None or test_encodings is None:\n",
        "    raise ValueError(\"Tokenization failed for one or more datasets.\")\n",
        "\n",
        "# Tokenized encodings to verify\n",
        "print(f\"Keys of train_encodings: {train_encodings.keys()}\")\n",
        "print(f\"Keys of valid_encodings: {valid_encodings.keys()}\")\n",
        "print(f\"Keys of test_encodings: {test_encodings.keys()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jebLf2lGUujJ",
        "outputId": "331b5c0e-6359-4090-fa8a-e1da0cc1adaa"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of train_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of valid_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of test_encodings: dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": train_encodings.input_ids, \"attention_mask\": train_encodings.attention_mask}, train_labels)).shuffle(len(train_texts)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": valid_encodings.input_ids, \"attention_mask\": valid_encodings.attention_mask}, valid_labels)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": test_encodings.input_ids, \"attention_mask\": test_encodings.attention_mask}, test_labels)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "P2vGFEo6odzP"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the input dictionary keys to match the expected names in the model\n",
        "def adjust_keys(encodings):\n",
        "    return {\n",
        "        'input_ids_layer': encodings['input_ids'],\n",
        "        'attention_mask_layer': encodings['attention_mask']\n",
        "    }"
      ],
      "metadata": {
        "id": "HojlgE13gHGw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply adjust_keys to each encoding\n",
        "train_encodings_adjusted = adjust_keys(train_encodings)\n",
        "valid_encodings_adjusted = adjust_keys(valid_encodings)\n",
        "test_encodings_adjusted = adjust_keys(test_encodings)\n",
        "\n",
        "# Print the Adjusted keys of train_encodings\n",
        "print(f\"Adjusted keys of train_encodings: {train_encodings_adjusted.keys()}\")\n",
        "print(f\"Adjusted keys of valid_encodings: {valid_encodings_adjusted.keys()}\")\n",
        "print(f\"Adjusted keys of test_encodings: {test_encodings_adjusted.keys()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJu9SL54jEaJ",
        "outputId": "3df57fe6-b082-4dc7-a26a-4baa27c9fa6b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted keys of train_encodings: dict_keys(['input_ids_layer', 'attention_mask_layer'])\n",
            "Adjusted keys of valid_encodings: dict_keys(['input_ids_layer', 'attention_mask_layer'])\n",
            "Adjusted keys of test_encodings: dict_keys(['input_ids_layer', 'attention_mask_layer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask}, train_labels)).shuffle(len(train_texts)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask}, valid_labels)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": test_encodings.input_ids, \"attention_mask_layer\": test_encodings.attention_mask}, test_labels)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "H9Cr-SWHn8hw"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check TensorFlow Dataset Elements\n",
        "for batch in test_dataset.take(1):\n",
        "    inputs, labels = batch\n",
        "    print(f\"Input IDs shape: {inputs['input_ids_layer'].shape}\")\n",
        "    print(f\"Attention Mask shape: {inputs['attention_mask_layer'].shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctvoX7tEpftA",
        "outputId": "a37d66c6-cca2-4e5f-d7d6-5b5d2e4463df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: (16, 50)\n",
            "Attention Mask shape: (16, 50)\n",
            "Labels shape: (16,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**"
      ],
      "metadata": {
        "id": "8mE-Nwt_Bece"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_roberta_cl_model(model, num_classes=6, dropout=0.3, learning_rate=0.0001):\n",
        "    model.trainable = False\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    model_inputs = [input_ids, attention_mask]\n",
        "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    model_out = model_out.last_hidden_state\n",
        "\n",
        "    conv = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(model_out)\n",
        "    conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
        "    conv = tf.keras.layers.Dropout(dropout)(conv)\n",
        "    lstm = tf.keras.layers.LSTM(units=256, return_sequences=False, return_state=False)(conv)\n",
        "    lstm = tf.keras.layers.Dropout(dropout)(lstm)\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(lstm)\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[classification])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=0.1)\n",
        "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "    classification_model.compile(optimizer=optimizer,\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n",
        "\n"
      ],
      "metadata": {
        "id": "txbHbODKTL0H"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "roberta_cl_model = create_roberta_cl_model(model=roberta_model, num_classes=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Rhw5KBZ-6q",
        "outputId": "dff7af3b-3d0e-44eb-9481-57984c78e3df"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# Clear session and free memory before starting the training\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-mDUCxrNHqS",
        "outputId": "4e3c4d5a-5040-4ed6-931f-f51a50d5aeec"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33955"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_cl_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8KiUq6leTpL",
        "outputId": "906cf2be-4974-466c-df95-015e48713cd9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLaye  [(None, 50)]                 0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (Inpu  [(None, 50)]                 0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1246456   ['input_ids_layer[0][0]',     \n",
            " aModel)                     ngAndCrossAttentions(last_   32         'attention_mask_layer[0][0]']\n",
            "                             hidden_state=(None, 50, 76                                           \n",
            "                             8),                                                                  \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 48, 256)              590080    ['tf_roberta_model[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 24, 256)              0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 24, 256)              0         ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)        (None, 256)                  0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 6)                    1542      ['dropout_38[0][0]']          \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125762566 (479.75 MB)\n",
            "Trainable params: 1116934 (4.26 MB)\n",
            "Non-trainable params: 124645632 (475.49 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask}, train_labels))\n",
        "# dataset = dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask}, valid_labels))\n",
        "vdataset = vdataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "VYR_wG0JkV9X"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_cl_model_history = roberta_cl_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j61jTAdytU0A",
        "outputId": "daf4005a-1d12-4d55-f909-c348179fe3c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2056/2056 [==============================] - 6704s 3s/step - loss: 0.7513 - accuracy: 0.7196 - val_loss: 0.4742 - val_accuracy: 0.8367\n",
            "Epoch 2/2\n",
            "2056/2056 [==============================] - 6686s 3s/step - loss: 0.4940 - accuracy: 0.8309 - val_loss: 0.4431 - val_accuracy: 0.8501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model\n",
        "preds = roberta_cl_model.predict(test_dataset)\n",
        "pred_labels = tf.argmax(preds, axis=-1)\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXrppfejZ6xm",
        "outputId": "2a188714-3762-48b7-fdac-15d7b6bf93a9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "254/254 [==============================] - 718s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94      1054\n",
            "           1       0.87      0.91      0.89      1125\n",
            "           2       0.95      0.74      0.83      1091\n",
            "           3       0.71      0.90      0.79       609\n",
            "           4       0.59      0.60      0.60       159\n",
            "           5       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.86      4061\n",
            "   macro avg       0.67      0.68      0.67      4061\n",
            "weighted avg       0.86      0.86      0.86      4061\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxWNXE9KZ60q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQMEMzEzI5OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9HAr9gQI5RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5l0ZUn5I5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hu6uSUCkI5Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JiC0EjCyI5cH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}