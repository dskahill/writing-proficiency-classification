{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHCWTR895IgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0c7d6a-804d-42a2-a8ee-93781694fd2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4CIMAWZwlYC",
        "outputId": "30d81aa6-113c-4284-c342-31fde42460a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (67.7.2)\n",
            "Installing collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.15.0 textstat-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import gc\n",
        "import textstat\n",
        "\n",
        "file_path = '/content/drive/My Drive/266 Project/efcamdat_sub.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poskK20S6plq",
        "outputId": "f195b1be-ce18-421d-eb13-27455b39fe29"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id  level  unit  learner_id learner_nationality  grade  \\\n",
            "0  679604      7     3      114335                  br     90   \n",
            "1  151196      9     2      136139                  sa     94   \n",
            "2  117084      9     4       34715                  br     88   \n",
            "3  113857      7     6       90269                  fr     90   \n",
            "4   22083      9     3       48465                  br     94   \n",
            "\n",
            "                      date  topic_id  \\\n",
            "0  2013-11-09 20:50:31.707        51   \n",
            "1  2012-09-25 06:01:08.117        66   \n",
            "2  2011-08-28 08:01:15.677        68   \n",
            "3  2011-07-31 14:51:22.547        54   \n",
            "4  2011-08-31 16:41:04.210        67   \n",
            "\n",
            "                                                text  cefr_numeric  \\\n",
            "0  From:l AS xxx@hotmail.com To: AS xxx@IXW.corpo...             3   \n",
            "1  I am so glad to receive this email from you. A...             3   \n",
            "2  Hi Fun Skydive, so I give up of my idea. I und...             3   \n",
            "3  Dear James, Some serious problems have been br...             3   \n",
            "4  Dear Sue, Thank you to interest in our product...             3   \n",
            "\n",
            "   cefr_grouped  \n",
            "0             2  \n",
            "1             2  \n",
            "2             2  \n",
            "3             2  \n",
            "4             2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check data**"
      ],
      "metadata": {
        "id": "iBD4pUDwa531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPyteXVE_sd",
        "outputId": "e0229ced-bdbf-4142-ca68-373110dbf784"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'level', 'unit', 'learner_id', 'learner_nationality', 'grade',\n",
              "       'date', 'topic_id', 'text', 'cefr_numeric', 'cefr_grouped', 'labels'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFLKk4FKFQoW",
        "outputId": "69d669a2-9f1d-41ac-c65e-afe1c4f0d69b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "377967"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values\n",
        "df['labels'] = df['cefr_numeric'].apply(lambda x: x-1)"
      ],
      "metadata": {
        "id": "83ZJX73Z2TMf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out invalid labels\n",
        "df = df[df['labels'].isin(range(6))]"
      ],
      "metadata": {
        "id": "KnMaeqfW7f-7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values after filtering\n",
        "print(\"Unique label values after filtering:\", df['labels'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ew-wbO2TXf",
        "outputId": "47bce52e-4c01-4ced-942a-e5ac57f42303"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values after filtering: [2 1 0 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "_G-sPwBgFWCs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation and test set sizes\n",
        "test_size = int(0.1 * len(df))  # 10% for testing\n",
        "valid_size = int(0.2 * len(df))  # 20% for validation"
      ],
      "metadata": {
        "id": "5lAedvCtwHB2"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split off the test set\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=test_size, shuffle=True, random_state=42)\n",
        "\n",
        "# Split off the validation set from the remaining training data\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=valid_size, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "Br3teBflFh06"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique label values in training set\n",
        "print(\"Unique label values in training set:\", train_labels.unique())\n",
        "print(\"Unique label values in validation set:\", valid_labels.unique())\n",
        "print(\"Unique label values in test set:\", test_labels.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T4BSTSW_Yw6",
        "outputId": "58955ccf-33c5-4d5a-c37f-aa834a092511"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values in training set: [2 0 1 3 4 5]\n",
            "Unique label values in validation set: [2 0 3 1 4 5]\n",
            "Unique label values in test set: [5 1 2 3 0 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Divide the data into training, validation, and test sets**"
      ],
      "metadata": {
        "id": "lrUsyA9NdhxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the size of the training and validation sets for faster processing\n",
        "train_texts = train_texts[:500]\n",
        "train_labels = train_labels[:500]\n",
        "valid_texts = valid_texts[:100]\n",
        "valid_labels = valid_labels[:100]\n",
        "test_texts = test_texts[:100]\n",
        "test_labels = test_labels[:100]"
      ],
      "metadata": {
        "id": "vKGNQfQ66qTa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract ARI readability scores\n",
        "def extract_ari_features(texts):\n",
        "    ari_scores = [textstat.automated_readability_index(text) for text in texts]\n",
        "    return ari_scores\n"
      ],
      "metadata": {
        "id": "0ZXCYIHCPIGU"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ari_scores = extract_ari_features(train_texts)\n",
        "valid_ari_scores = extract_ari_features(valid_texts)\n",
        "test_ari_scores = extract_ari_features(test_texts)"
      ],
      "metadata": {
        "id": "q-DcquR6PIPs"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "max_length = 50\n",
        "rtokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "0Cw1fj_Ew3Px"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_length):\n",
        "    if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "        raise ValueError(\"Input texts should be a list of strings.\")\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "JseLIcudw6oe"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_tokenize(texts, tokenizer, max_length):\n",
        "    try:\n",
        "        encodings = tokenize_texts(texts, tokenizer, max_length)\n",
        "        if 'input_ids' not in encodings:\n",
        "            raise ValueError(\"Tokenization did not produce 'input_ids'.\")\n",
        "        print(f\"Successfully tokenized {len(texts)} texts.\")\n",
        "        return encodings\n",
        "    except ValueError as e:\n",
        "        print(f\"Tokenization error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "BGP0hi02w6wz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = safe_tokenize(list(train_texts), rtokenizer, max_length)\n",
        "valid_encodings = safe_tokenize(list(valid_texts), rtokenizer, max_length)\n",
        "test_encodings = safe_tokenize(list(test_texts), rtokenizer, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxezpDYQw69T",
        "outputId": "1bb174d4-be08-4eb3-be39-f414c4a40492"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized 500 texts.\n",
            "Successfully tokenized 100 texts.\n",
            "Successfully tokenized 100 texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_encodings is None or valid_encodings is None or test_encodings is None:\n",
        "    raise ValueError(\"Tokenization failed for one or more datasets.\")\n",
        "\n",
        "# Print the keys of the tokenized encodings to verify\n",
        "print(f\"Keys of train_encodings: {train_encodings.keys()}\")\n",
        "print(f\"Keys of valid_encodings: {valid_encodings.keys()}\")\n",
        "print(f\"Keys of test_encodings: {test_encodings.keys()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flCVqMlaxHMr",
        "outputId": "8f4d7f52-c9e4-4129-d7ba-eac9cb13f012"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of train_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of valid_encodings: dict_keys(['input_ids', 'attention_mask'])\n",
            "Keys of test_encodings: dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Datasets with ARI scores\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask, \"ari_score\": train_ari_scores},\n",
        "    train_labels\n",
        ")).shuffle(len(train_texts)).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask, \"ari_score\": valid_ari_scores},\n",
        "    valid_labels\n",
        ")).batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids_layer\": test_encodings.input_ids, \"attention_mask_layer\": test_encodings.attention_mask, \"ari_score\": test_ari_scores},\n",
        "    test_labels\n",
        ")).batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "-NUCrWL3xHU_"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check TensorFlow Dataset Elements\n",
        "for batch in test_dataset.take(1):\n",
        "    inputs, labels = batch\n",
        "    print(f\"Input IDs shape: {inputs['input_ids_layer'].shape}\")\n",
        "    print(f\"Attention Mask shape: {inputs['attention_mask_layer'].shape}\")\n",
        "    print(f\"ARI Score shape: {inputs['ari_score'].shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    print(f\"Label values: {labels.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p12VLhQoxHXi",
        "outputId": "2276ac8a-00b7-465b-f11a-72f31561e180"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: (16, 50)\n",
            "Attention Mask shape: (16, 50)\n",
            "ARI Score shape: (16,)\n",
            "Labels shape: (16,)\n",
            "Label values: [5 1 1 2 3 2 1 0 0 3 0 0 4 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model architecture**"
      ],
      "metadata": {
        "id": "l20BGOsIE-Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def create_roberta_cl_model(model, num_classes=6, dropout=0.3, learning_rate=0.0001):\n",
        "    model.trainable = False\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "    ari_score = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='ari_score')\n",
        "\n",
        "    model_inputs = [input_ids, attention_mask, ari_score]\n",
        "    model_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    model_out = model_out.last_hidden_state\n",
        "\n",
        "    conv = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(model_out)\n",
        "    conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
        "    conv = tf.keras.layers.Dropout(dropout)(conv)\n",
        "    lstm = tf.keras.layers.LSTM(units=256, return_sequences=False, return_state=False)(conv)\n",
        "    lstm = tf.keras.layers.Dropout(dropout)(lstm)\n",
        "\n",
        "    # Concatenate LSTM output with the ARI score\n",
        "    concatenated = tf.keras.layers.Concatenate()([lstm, ari_score])\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax', name='classification_layer')(concatenated)\n",
        "\n",
        "    classification_model = tf.keras.Model(inputs=model_inputs, outputs=classification)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=0.1)\n",
        "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "    classification_model.compile(optimizer=optimizer,\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model"
      ],
      "metadata": {
        "id": "bt9I4AIbPIV3"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "roberta_cl_model = create_roberta_cl_model(model=roberta_model, num_classes=6)"
      ],
      "metadata": {
        "id": "1HT7DcTbPIY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b8e2a7-fc01-4dca-cba2-0de85f613d7e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear session and free memory before starting the training\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu7TZwPWxWt2",
        "outputId": "699cbccc-b542-4358-808e-168991c856e0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37919"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model summary\n",
        "roberta_cl_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWYVZ_M6xbTp",
        "outputId": "d27832e9-78bb-4bb1-a4c4-7246bab5a546"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids_layer (InputLaye  [(None, 50)]                 0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " attention_mask_layer (Inpu  [(None, 50)]                 0         []                            \n",
            " tLayer)                                                                                          \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1246456   ['input_ids_layer[0][0]',     \n",
            " aModel)                     ngAndCrossAttentions(last_   32         'attention_mask_layer[0][0]']\n",
            "                             hidden_state=(None, 50, 76                                           \n",
            "                             8),                                                                  \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 48, 256)              590080    ['tf_roberta_model[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 24, 256)              0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 24, 256)              0         ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)        (None, 256)                  0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " ari_score (InputLayer)      [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 257)                  0         ['dropout_38[0][0]',          \n",
            "                                                                     'ari_score[0][0]']           \n",
            "                                                                                                  \n",
            " classification_layer (Dens  (None, 6)                    1548      ['concatenate[0][0]']         \n",
            " e)                                                                                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125762572 (479.75 MB)\n",
            "Trainable params: 1116940 (4.26 MB)\n",
            "Non-trainable params: 124645632 (475.49 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBGlUneh1YfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": train_encodings.input_ids, \"attention_mask_layer\": train_encodings.attention_mask}, train_labels))\n",
        "# dataset = dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices(({\"input_ids_layer\": valid_encodings.input_ids, \"attention_mask_layer\": valid_encodings.attention_mask}, valid_labels))\n",
        "vdataset = vdataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "-y80hYeD1Yu0"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "roberta_cl_model_history = roberta_cl_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScQZl-BxqIC",
        "outputId": "69d310fc-b736-49d9-e212-8355f8261ca2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 165s 4s/step - loss: 1.7911 - accuracy: 0.2680 - val_loss: 1.4183 - val_accuracy: 0.3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate the model\n",
        "preds = roberta_cl_model.predict(test_dataset)\n",
        "pred_labels = tf.argmax(preds, axis=-1)\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "metadata": {
        "id": "ltk4d99FxqWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04707617-74bb-4db1-e94b-605cf7c1e592"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 40s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.19      0.31        26\n",
            "           1       0.00      0.00      0.00        24\n",
            "           2       0.25      0.68      0.37        25\n",
            "           3       0.27      0.39      0.32        18\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.29       100\n",
            "   macro avg       0.23      0.21      0.17       100\n",
            "weighted avg       0.33      0.29      0.23       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqrxeeR4xqYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hFTdqlCxqb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}